---
title: "Chapter 10: Lectal Lectometry"
author: "Stefano De Pascale"
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
    number_sections: true
    css: ./sources/styles.css
    md_extensions: +bracketed_spans
bibliography: ./sources/bibliography.bib
csl: ./sources/apa.csl

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tippy)
library(knitr)
library(rmdformats)
library(kfigr)
library(kableExtra)
library(tidyverse)
library(pander)
library(DT)

d.clust <- read.csv("C://Users/u0102617/Box/Rmarkdown/concepts-measures-withclusters.txt", header=T, sep=" ", check.names = F)
d.noclust <- read.csv("C://Users/u0102617/Box/Rmarkdown/concepts-measures-withoutclusters.txt", header=T, sep=" ", check.names = F)

```
# Weighting strategies for lectometric analyses

## The dataset: selection of concepts

```{r, include=FALSE}
concepts1 <- c("safety belt", "ecstasy", "wedding", "display window", "apartment building", "chat", "religion", "appreciation", "stress", "individuality")
concepts2 <- c("reflection", "demolition", "keyboard", "dip", "repayment", "stronghold", "nail", "sex", "palm tree", "pitch")
concepts3 <- c("peace talks", "submarine", "retaliation", "town center", "pity", "allegation", "tour operator", "filmmaker", "suspicion", "draw")
concepts4 <- c("commuter", "opening hours", "lack of space"," electoral district", "cash", "scriptwriter", "voyage of discovery", "amusement park", "database", "nuclear reactor")
```

The set of analyzed concepts were all selected from the lexical variables generated by the enhanced Clustering-by-Committee algorithm (henceforth CBC 2.0). In order to qualify for selection we required the lexical variable to match in exact terms (i.e. in composition and in size) with the Dutch WordNet synset that is incorporated in Cornetto 1.3 (the release available in R2d2). We can distinguish four subgroups in this set of concepts, based on 1) the potential presence of polysemy in one its constituent lexicalisations and 2) the number of lexicalisations:

* 10 concepts with 3 near-synonyms, one of which appears in more than 1 synset, and is therefore considered potentially polysemous. These concepts are: <span style="text-transform: lowercase; font-variant: small-caps;">`r paste0("[",concepts1,"](#",sub(" ","",concepts1),")", collapse = ", ")`</span>. They were chosen among the 16 exact matches with the aforementioned properties, and in addition by avoiding the extremely frequent concepts (more than 10000 tokens, before disambiguation).

* 10 concepts with 2 near-synonyms, one of which appears in more than 1 synset, and is therefore considered potentially polysemous. These concepts are: <span style="text-transform: lowercase; font-variant: small-caps;">`r paste0("[",concepts2,"](#",sub(" ","",concepts2),")", collapse = ", ")`</span>. As the exact matches with 2 near-synonyms constitute a sizeable set in the CBC output (450), we selected concepts such that A) they would have a total frequency lower than 1500, B) their set would vary along a large range of external uniformity values (actually from 0.5 to 0.9) and C) their polysemy structure was expected to emerge from the token-based vector spaces.

* 11 concepts with 3 near-synonyms, of which none appears in more than 1 synsets. Therefore, we do not expect any of these variants to show a traditionally-defined polysemous  `r tippy("structure", tooltip="<sup>1</sup>This does not mean that these variants cannot display other, more or less marked and unexpected semasiological variation of other sorts, for instance, oblique construal, or topic-specific readings", theme="light-border", arrow=TRUE)`^1^. These concepts are: <span style="text-transform: lowercase; font-variant: small-caps;">`r paste0("[",concepts3,"](#",sub(" ","",concepts3),")", collapse = ", ")`</span>. To these initial 10 concepts we added an 11th one, namely <span style="text-transform: lowercase; font-variant: small-caps;">[convict](#convict)</span>. They were chosen among the 16 exact matches with the aforementioned properties.

* 11 concepts with 2 near-synonyms, of which none appears in more than 1 synsets. These concepts are: <span style="text-transform: lowercase; font-variant: small-caps;">`r paste0("[",concepts4,"](#",sub(" ","",concepts4),")", collapse = ", ")`</span>. To these initial 10 concepts we added an 11th one, namely <span style="text-transform: lowercase; font-variant: small-caps;">[living room](#livingroom)</span>. As the exact matches with 2 near-synonyms constitute a sizeable set in the CBC output (450), we selected concepts such that A) they would have a total frequency lower than 1500, B) their set would vary along the full spectrum of external uniformity values (actually from 0.1 to 0.9) and C) their polysemy structure was expected to emerge from the token-based vector spaces.

## Weighting and filtering of concepts (with or without cluster analysis)

Different concepts can contribute in different ways to the aggregated lectal distances between the national varieties of Dutch, that is, Belgian Dutch and Netherlandic Dutch. In the <span class="reference">Geeraerts, Grondelaers, & Speelman (1999)</span> monograph, the procedure for determining a differential contribution of concepts consisted of three steps:

* first, to establish whether the lectal profiles for a given concept are significantly different,
* second, to include such differences in the aggregate calculation over concepts when they are significant, and to include the nonsignificant cases as exhibiting 100% uniformity or equivalently 0 distance (this means, in other words, that the significance test is precisely that: a test of the hypothesis that the observed differences represent ‘real’ differences),
* third, when aggregating over different concepts, to give a different weight to the uniformity measure of given concept on the basis of its relative frequency.

The original 1999 method, and its subsequent refinements, would apply to concepts in their entirety, often after manual semantic disambiguation of the tokens had been carried out on the concept under scrutiny. Applying this same procedure to clusters within a concept would mean the following:

* first, to establish whether the cluster-based lexical profiles of a given concept are significantly different;
* second, to treat non-significant clusters as exhibiting 100% uniformity or equivalently 0 distance and to treat significantly different clusters as exhibiting ‘real’ differences;
* third, to aggregate over the uniformity measures of different clusters weighted by the relative frequency of clusters.

The first and second parts of this method thus essential relies on **stastical significance as a filter mechanism**. When we approach the comparison of profiles within the statistical framework of hypothesis testing, we can consider the frequencies in a profile to be the sample of a random variable that has a multinomial distribution. The null hypothesis of such test states that both samples are drawn from the same population: the distribution of the synonyms does not differ between the national varieties, and, based on that, the inference would be that the national varieties cannot be considered different linguistic varieties. The alternative hypothesis consists of claiming that the onomasiological profiles do differ, and that we are dealing with separate linguistic varieties. A statistical test can then calculate a probability (i.e. the p-value) that quantifies how likely it is to find the observed differences in distribution of near-synonyms, given that the null hypothesis of no lectal differences holds. One rejects the null hypothesis and accepts the alternative hypothesis when this probability is lower than the customary significance level α = 0.05, which is the probability that we have chosen under which we reject the null hypothesis. The strength of statistical testing approach is that it is sensitive to how much evidence there is for the assumption that there actually is an underlying difference between the two profiles, so that we can avoid overrating distances that are not based on a substantial amount of tokens. 

<div class="infobox", font-size="10px">
  Statistical significance can also be employed in two other different ways: as a <strong>weighting measure</strong> or as a <strong>distance metric</strong> by itself. In the first case, instead of using the the p-value as a cut-off point for filtering, one could weight (i.e. multiply) the uniformity or distance by 1 - p-value, such that overall significantly different onomasiological profiles could have a larger impact on the aggregate lectal distance between Netherlandic Dutch and Belgian Dutch. This approach was suggested in <span class="reference">Speelman, Grondelaers, & Geeraerts (2003: 366)</span> and explored in <span class="reference">Ruette (2012: 93)</span>. The influence of this type of weighting was nevertheless considered to be minimal. In the second case the p-value obtained after the statistical significance test would be used directly as a distance metric. This option was investigated in <span class="reference">Speelman, Grondelaers, & Geeraerts (2003)</span>. In this report we will limit ourselves to using statistical significance as a filter mechanism. 
</div>

Two statistical tests are going to be used in this respect: the **Log Likelihood Ratio test** and **Fisher's Exact Test**. If the resulting p-value of such a test is lower than the customary significance level α = 0.05, the null hypothesis of the absence of difference between profiles is rejected, and the observed City Block distance can be safely assumed to show a real difference between the regiolects. If the p-value is higher than the α-level, the City Block distance is set to 0, no lectal distance, which corresponds to accepting the null hypothesis of no difference between the onomasiological profiles. Although the Log Likelihood Ratio test, and in general hypothesis testing, was precisely meant to avoid taking for granted lectal distances based on low-frequent profiles, it is a test that can still suffer from sparse data. Fisher's Exact Test is known to suffer less from data sparseness in the cells of the contigency tables, and should therefore be even more reliable than the LLR for low-frequent profiles.  

The weakness of this approach is that statistical significance can be manipulated by simply increasing the sample size of one or both onomasiological profiles in the comparison. Being dependent on sample size, the test will exaggerate the weight of frequent concepts, regardless of the lectal distance (in other words, the effect size) and downplay the weight of infrequent concepts (this criticism was already raised by <span class="reference">Ruette (2012: 93)</span>. 

To overcome this weakness, we can rely on **statistical power as an (additional) filter mechanism**. The statistical power of one of the binary hypothesis tests introduced above indicates, loosely speaking, the confidence we have that with the available data we are able to find a 'real' difference between profiles, given the presence of such a 'real' difference (i.e given that we have rejected the null hypothesis and accepted the alternative hypothesis). The power of a test is directly related to the type II-error probability β. The β-level of a test indicates the probability (we can tolerate) of falsely rejecting the true alternative hypothesis, and accepting the false null hypothesis. The corresponding statistical power of a test is 1 − β, and ranges from 0 to 1. Keeping the type II-error probability low means increasing the power of a study. Just as with the significance level α (which is the probability we can tolerate of falsely rejecting the true null hypothesis and accepting the false alternative hypothesis, that is, a the Type I-error), a threshold can be chosen in advance by the researcher, and the minimal power of a test this is usually set to 0.80. The computation of the statistical power depends on a combination of the significance level (α-level), the sample size and the effect size (i.e. the lectal distance). 

It is indeed possible to observe a significant difference in profiles between regiolects (where the p-value is lower than the significance level α), but that significance might be either due to the sample size (with the idea that the larger the sample size, the easier it will be to obtain a significant effect, regardless of the magnitude of the effect) or  to a genuine, true difference between the regiolects. A power analysis is often conducted prior to the hypothesis stest, in order to calculate the minimum sample size required for observing an effect of a given size. But power can be estimated even post-hoc after a test has been conducted. Using statistical power as a filter mechanism then boils down to a similar procedure as with statistical significance: if the 1 - β is lower than the 0.80, meaning there is insufficient power to accept the alternative hypothesis, the City Block distance is set to 0, no lectal distance, which corresponds to accepting the null hypothesis of no difference between the onomasiological profiles.

<div class="infobox", style="float:none; font-size:16px; color:red; border: 4px solid red; width: 50%; margin: auto">
<strong>WARNING!</strong> It is questionable whether this observed power can be a useful additional filter mechanism, or whether it is simply an unsound concept. I leave it up here for discussion! 
</div>

## Baseline scenario: lectal distances without semantic control

Before we venture in the calculation of lectal distances based on subregiones/clusters within the semantic space defined by a concept's lexicalizations, it might instructive to look at lectal distances generated without any regard for the semantic structure of the lexemes involved. Lectal distances based on all retrieved tokens of the lexemes can then act as useful reference to which we can compare lectal distances obtained with more attention for the semantic reality behind those tokens.

It is evident that these calculations are flawed, because we know for sure that they are flawed: there has been no correction or selection of the tokens whatsoever. Although they have a rather limited use for description, they can showcase how the applying the different filters (significance based on LLR, based on Fisher's Exact Test, based on power) yield different outcomes, and how the real analyses will/might look like. 

```{r, echo=FALSE}
d.concept <- d.noclust %>% 
  select('concept','distance_cityblock', 'filter.fisher','filter.llr','filter.power') %>%
  mutate(distance_cityblock*filter.llr,
         distance_cityblock*filter.fisher,
         distance_cityblock*filter.power) %>%
  select(-starts_with('filter'))

colnames(d.concept) <- c("Concept", "No filter", "LLR filter", "Fisher filter","power filter")

# kable(d.concept, caption= "No clusters", digits = 3, escape = FALSE) %>% 
#   kable_styling(full_width = T) %>%
#   add_header_above(c("","City Block"=4)) %>%
#   column_spec(c(1:5), extra_css = "vertical-align:middle;")

sketch = htmltools::withTags(table(
  class = 'display',
  thead(
    tr(
      th(rowspan = 2, 'Concepts'),
      th(colspan = 4, 'City Block'),
    ),
    tr(
      lapply(rep(c("No filter", "LLR filter", "Fisher filter","power filter"), 1), th)
    )
  )
))

d.concept$Concept <- str_replace_all(d.concept$Concept,"_"," ")

datatable(d.concept, rownames=FALSE, container = sketch, filter="top") %>%
  formatStyle('Concept', c('LLR filter','power filter'), target="row", 
              fontVariant="small-caps", 
              fontSize="16px", 
              fontWeight="bold", color=styleEqual(0, 'red')) %>%
  formatStyle('No filter', background = styleColorBar(d.concept$`No filter`, '#6699FF'))

```

The main finding resulting from the table above is that for the vast majority of concepts (36 out of 42 to be precise), the application of filters does not affect the lectal distances. For almost all concepts the hypothesis tests, be it LLR or Fisher's Exact test, report significantly different distributions; therefore most City Block distances are kept. The concepts for which te tests cannot reject the null hypothesis of no lectal difference, and thus receive a distance of 0, are <span style="text-transform: lowercase; font-variant: small-caps;">[nuclear reactor](#nuclearreactor), [suspicion](#suspicion) and [religion](#religion)</span>. As these are all concepts that report the lowest lectal distances in the dataset, 




## safety belt {#safetybelt}

```{r, anchor = "Table", echo=FALSE}

l <- "safety_belt"
d.concept <- d.clust %>% 
  filter(concept == l) %>%
  select('cluster', 'distance_cityblock', 'filter.fisher','filter.llr','filter.power') %>%
  mutate(distance_cityblock*filter.llr,
         distance_cityblock*filter.fisher,
         distance_cityblock*filter.power) %>%
  select(-starts_with('filter'))

d.concept$semantics <- c(
"riem as 'oar/paddle', more in the idiom 'roeien met de riemen die men heeft'",
"<ul class='intable'><li>riem as 'shoulder/waistbelt', more specifically in the idiom 'een hart onder de riem steken'</li><li>gordel as 'belt', more specifically in the idioms 'een slag/stoot onder de gordel'</li></ul>",
"'safety belt'",
"<ul class='intable'><li>gordel as 'waistbelt', often as 'judo/karate belt'</li><li>riem as 'waistbelt', often in fashion context (with leren)</li></ul>",
"<ul class='intable'><li>gordel as 'encircling area' (e.g. groene gordel) and as 'safety belt'</li><li>riem as 'waistbelt', more specifically in the idiom 'de riem afleggen' and in fashion context</li></ul>")

tableList <- list()
for (cl in 1:5) {
  
  freqs <- d.clust %>% 
    filter(concept == l & cluster == cl) %>%
    select(starts_with('var'))
  
  freqs.m <- matrix(freqs %>% select(-variants), ncol=2)
  
  variants <- as.character(freqs$variants)
  
  colnames(freqs.m) <- c("BE","NL")
  rownames(freqs.m) <- str_split(variants,"/")[[1]]
  
  tableList[[cl]] <- kable(freqs.m)
}

d.concept <- d.concept %>% add_column(table=tableList, .before="distance_cityblock")


colnames(d.concept) <- c("Cluster", "Contigency table", "No filter", "LLR filter", "Fisher filter","power filter","Semantics")

kable(d.concept, caption= str_c("Definitions of '", l, "'."), digits = 3, escape = FALSE) %>% 
  kable_styling(full_width = T) %>%
  add_header_above(c("","","City Block"=4,"")) %>%
  column_spec(c(1:7), extra_css = "vertical-align:middle;")
```

# Stability analyses

# A note on weighting and clusters

Why would we want to treat clusters differently? What is the logic behind weighting of clusters? Treating clusters differently in the calculation of uniformity measures across lects could be relevant for two reasons: because some clusters are conceptually more peripheral to the concept, or because the onomasiological profiles for different clusters are different. This note only explores the second case.

The recognition that the onomasiological profiles for different clusters may be different leads to a fairly straightforward way of treating the clusters, a procedure that is fully parallel with the treatment of entire concepts in the original 1999 method. To recall, the 1999 approach involves,

* first, to establish whether the lectal profiles for a given concept are significantly different,
* second, to include such differences in the aggregate calculation over concepts when they are significant, and to include the nonsignificant cases as exhibiting 100% uniformity (this means, in other words, that the significance test is precisely that: a test of the hypothesis that the observed differences represent ‘real’ differences),
* third, when aggregating over different concepts, to give a different weight to the uniformity measure of given concept on the basis of its relative frequency.

Applying this procedure to clusters within a concept would mean the following:

* first, to establish whether the cluster-based lexical profiles of a given concept are significantly different;
* second, to treat non-significant clusters as exhibiting hundred percent uniformity and to treat significantly different clusters as exhibiting ‘real’ differences;
* third, to aggregate over the uniformity measures of different clusters weighted by the relative frequency of clusters.

Two further questions arise.

First, do we still need aggregation over concepts when we aggregate over clusters first? Not necessarily, but proceeding directly to an aggregation over all clusters of all concepts is only admissable when the relative frequency of clusters that is used as a weighting measure is calculated on the entire database and not just within the concept. Also, since we would still be interested in the uniformity measures for different concepts as such, not much would be gained by abandoning a two-step procedure (in which we work with relative frequencies within a concept first and next proceed with the relative frequency of the concept).

Second, what are the relevant variants to this procedure? The weighting of clusters, like the weighting of concepts in the original 1999 procedure, is based on the reasoning that contexts that are communicatively less important should play a lesser role in aggregate calculations of uniformity. In terms of concepts, the logic is as follows. In the form of onomasiological profiles for concept x in lects A and B, we know the probability that an A-lecter and a B-lecter would be using the same terms when talking about x. Next, when aggregating over x and y, we take into account that the probability that they might talk about x is lower than the probability they might talk about y. That probability is a combination of the inclination of an A-lecter to talk about x and the inclination of a B-lecter to talk about x. In this perspective, it doesn’t matter all that much that these inclinations might differ: what we’re interested in is the overall communicative incidence of talk about x compared to talk about y.

Still, when the relative frequencies of x in A and B differ, an asymmetric approach may be considered, in which the weighting of concepts is based on their frequency distribution in A alone or in B alone. The intended construct could then be paraphrased as: ‘If we pretend that B-lecters talk about x (and y and z etc.) as frequently as A-lecters, how would that affect the overall uniformity between A and B (and conversely from the point of view of B)?’. The extrapolation from concepts to clusters is straightforward.

It is not immediately obvious how to interpret such a perspective (and accordingly evaluate its relevance), but one way of doing this would be to think about it in terms of ‘culture’. If culture (or at least ‘communicative culture, culture as discourse’) is defined in terms of the relative importance with which certain topics are discussed, an asymmetric weighting of concepts is a way of examining the effect of culture on linguistic uniformity: if the two lects had the same ‘culture’, either the A one or the B one, how would that influence the uniformity measures?

However, as so often with the measures we develop, the relevance of such a cultural perspective may be more diachronic than synchronic. While the ‘what if’ perspective is a little bit academic for synchronic data, for diachronic data, it would be downright relevant to examine whether an observed convergence or divergence of uniformity is influenced by a ‘cultural’ change in the relative frequencies of the concepts in the various lects (or in other words, whether it correlates with a convergence or divergence of the lectal communicative habits and preferences).

So, 

1. it is best to treat clusters according to the procedure for concepts, following the 1999 approach; 
2. a two-step approach (a weighted aggregation over clusters within concepts followed by a weighted aggregation over concepts) may be maintained; 
3. rather than transforming lectally asymmetric concept frequencies into synchronic weighting factors, a more insightful way of using them would be to include them in the analysis of diachronic processes of convergence and divergence.

At the same time, further thought may be given to the possibility of weighting clusters on the basis of their structural position within the concept.


Weighting and semantic lectometry  

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
